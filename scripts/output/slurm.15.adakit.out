/home/g/grimmj/miniconda3/envs/idiom/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.
Loaded pretrained model EleutherAI/pythia-1.4b into HookedTransformer
Running on device cuda.

Processing split:  formal
Map:   0%|          | 0/2761 [00:00<?, ? examples/s]Map:   0%|          | 1/2761 [00:04<3:39:25,  4.77s/ examples]Map:   0%|          | 2/2761 [00:06<2:27:08,  3.20s/ examples]Map:   0%|          | 3/2761 [00:09<2:13:58,  2.91s/ examples]Map:   0%|          | 4/2761 [01:01<17:09:06, 22.40s/ examples]Map:   0%|          | 5/2761 [01:16<14:57:54, 19.55s/ examples]Map:   0%|          | 6/2761 [01:18<10:23:01, 13.57s/ examples]Map:   0%|          | 7/2761 [01:24<8:33:01, 11.18s/ examples] Map:   0%|          | 8/2761 [01:32<7:40:55, 10.05s/ examples]Map:   0%|          | 9/2761 [01:34<5:52:43,  7.69s/ examples]Map:   0%|          | 10/2761 [01:45<6:32:10,  8.55s/ examples]Map:   0%|          | 11/2761 [02:59<22:03:15, 28.87s/ examples]Map:   0%|          | 12/2761 [03:10<17:50:42, 23.37s/ examples]Map:   0%|          | 13/2761 [03:17<14:00:55, 18.36s/ examples]Map:   1%|          | 14/2761 [03:27<12:04:42, 15.83s/ examples]Map:   1%|          | 15/2761 [03:37<10:44:27, 14.08s/ examples]Map:   1%|          | 16/2761 [04:12<15:33:38, 20.41s/ examples]Map:   1%|          | 17/2761 [04:17<12:04:01, 15.83s/ examples]