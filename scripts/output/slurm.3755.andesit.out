slurmstepd-andesit: error: Unable to create TMPDIR [/tmp/user/32202]: Permission denied
slurmstepd-andesit: error: Setting TMPDIR to /tmp
WARNING:root:float16 models may not work on CPU. Consider using a GPU or bfloat16.
WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.
WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping
Loaded pretrained model meta-llama/Llama-3.2-1B-Instruct into HookedTransformer

Model uses LayerNorm!

Running logit attribution on device cuda.

Processing split:  formal
Map:   0%|          | 0/1480 [00:00<?, ? examples/s]
Computing logit attribution for the following components:
['L0H0', 'L0H1', 'L0H2', 'L0H3', 'L0H4', 'L0H5', 'L0H6', 'L0H7', 'L0H8', 'L0H9', 'L0H10', 'L0H11', 'L0H12', 'L0H13', 'L0H14', 'L0H15', 'L0H16', 'L0H17', 'L0H18', 'L0H19', 'L0H20', 'L0H21', 'L0H22', 'L0H23', 'L0H24', 'L0H25', 'L0H26', 'L0H27', 'L0H28', 'L0H29', 'L0H30', 'L0H31', 'L1H0', 'L1H1', 'L1H2', 'L1H3', 'L1H4', 'L1H5', 'L1H6', 'L1H7', 'L1H8', 'L1H9', 'L1H10', 'L1H11', 'L1H12', 'L1H13', 'L1H14', 'L1H15', 'L1H16', 'L1H17', 'L1H18', 'L1H19', 'L1H20', 'L1H21', 'L1H22', 'L1H23', 'L1H24', 'L1H25', 'L1H26', 'L1H27', 'L1H28', 'L1H29', 'L1H30', 'L1H31', 'L2H0', 'L2H1', 'L2H2', 'L2H3', 'L2H4', 'L2H5', 'L2H6', 'L2H7', 'L2H8', 'L2H9', 'L2H10', 'L2H11', 'L2H12', 'L2H13', 'L2H14', 'L2H15', 'L2H16', 'L2H17', 'L2H18', 'L2H19', 'L2H20', 'L2H21', 'L2H22', 'L2H23', 'L2H24', 'L2H25', 'L2H26', 'L2H27', 'L2H28', 'L2H29', 'L2H30', 'L2H31', 'L3H0', 'L3H1', 'L3H2', 'L3H3', 'L3H4', 'L3H5', 'L3H6', 'L3H7', 'L3H8', 'L3H9', 'L3H10', 'L3H11', 'L3H12', 'L3H13', 'L3H14', 'L3H15', 'L3H16', 'L3H17', 'L3H18', 'L3H19', 'L3H20', 'L3H21', 'L3H22', 'L3H23', 'L3H24', 'L3H25', 'L3H26', 'L3H27', 'L3H28', 'L3H29', 'L3H30', 'L3H31', 'L4H0', 'L4H1', 'L4H2', 'L4H3', 'L4H4', 'L4H5', 'L4H6', 'L4H7', 'L4H8', 'L4H9', 'L4H10', 'L4H11', 'L4H12', 'L4H13', 'L4H14', 'L4H15', 'L4H16', 'L4H17', 'L4H18', 'L4H19', 'L4H20', 'L4H21', 'L4H22', 'L4H23', 'L4H24', 'L4H25', 'L4H26', 'L4H27', 'L4H28', 'L4H29', 'L4H30', 'L4H31', 'L5H0', 'L5H1', 'L5H2', 'L5H3', 'L5H4', 'L5H5', 'L5H6', 'L5H7', 'L5H8', 'L5H9', 'L5H10', 'L5H11', 'L5H12', 'L5H13', 'L5H14', 'L5H15', 'L5H16', 'L5H17', 'L5H18', 'L5H19', 'L5H20', 'L5H21', 'L5H22', 'L5H23', 'L5H24', 'L5H25', 'L5H26', 'L5H27', 'L5H28', 'L5H29', 'L5H30', 'L5H31', 'L6H0', 'L6H1', 'L6H2', 'L6H3', 'L6H4', 'L6H5', 'L6H6', 'L6H7', 'L6H8', 'L6H9', 'L6H10', 'L6H11', 'L6H12', 'L6H13', 'L6H14', 'L6H15', 'L6H16', 'L6H17', 'L6H18', 'L6H19', 'L6H20', 'L6H21', 'L6H22', 'L6H23', 'L6H24', 'L6H25', 'L6H26', 'L6H27', 'L6H28', 'L6H29', 'L6H30', 'L6H31', 'L7H0', 'L7H1', 'L7H2', 'L7H3', 'L7H4', 'L7H5', 'L7H6', 'L7H7', 'L7H8', 'L7H9', 'L7H10', 'L7H11', 'L7H12', 'L7H13', 'L7H14', 'L7H15', 'L7H16', 'L7H17', 'L7H18', 'L7H19', 'L7H20', 'L7H21', 'L7H22', 'L7H23', 'L7H24', 'L7H25', 'L7H26', 'L7H27', 'L7H28', 'L7H29', 'L7H30', 'L7H31', 'L8H0', 'L8H1', 'L8H2', 'L8H3', 'L8H4', 'L8H5', 'L8H6', 'L8H7', 'L8H8', 'L8H9', 'L8H10', 'L8H11', 'L8H12', 'L8H13', 'L8H14', 'L8H15', 'L8H16', 'L8H17', 'L8H18', 'L8H19', 'L8H20', 'L8H21', 'L8H22', 'L8H23', 'L8H24', 'L8H25', 'L8H26', 'L8H27', 'L8H28', 'L8H29', 'L8H30', 'L8H31', 'L9H0', 'L9H1', 'L9H2', 'L9H3', 'L9H4', 'L9H5', 'L9H6', 'L9H7', 'L9H8', 'L9H9', 'L9H10', 'L9H11', 'L9H12', 'L9H13', 'L9H14', 'L9H15', 'L9H16', 'L9H17', 'L9H18', 'L9H19', 'L9H20', 'L9H21', 'L9H22', 'L9H23', 'L9H24', 'L9H25', 'L9H26', 'L9H27', 'L9H28', 'L9H29', 'L9H30', 'L9H31', 'L10H0', 'L10H1', 'L10H2', 'L10H3', 'L10H4', 'L10H5', 'L10H6', 'L10H7', 'L10H8', 'L10H9', 'L10H10', 'L10H11', 'L10H12', 'L10H13', 'L10H14', 'L10H15', 'L10H16', 'L10H17', 'L10H18', 'L10H19', 'L10H20', 'L10H21', 'L10H22', 'L10H23', 'L10H24', 'L10H25', 'L10H26', 'L10H27', 'L10H28', 'L10H29', 'L10H30', 'L10H31', 'L11H0', 'L11H1', 'L11H2', 'L11H3', 'L11H4', 'L11H5', 'L11H6', 'L11H7', 'L11H8', 'L11H9', 'L11H10', 'L11H11', 'L11H12', 'L11H13', 'L11H14', 'L11H15', 'L11H16', 'L11H17', 'L11H18', 'L11H19', 'L11H20', 'L11H21', 'L11H22', 'L11H23', 'L11H24', 'L11H25', 'L11H26', 'L11H27', 'L11H28', 'L11H29', 'L11H30', 'L11H31', 'L12H0', 'L12H1', 'L12H2', 'L12H3', 'L12H4', 'L12H5', 'L12H6', 'L12H7', 'L12H8', 'L12H9', 'L12H10', 'L12H11', 'L12H12', 'L12H13', 'L12H14', 'L12H15', 'L12H16', 'L12H17', 'L12H18', 'L12H19', 'L12H20', 'L12H21', 'L12H22', 'L12H23', 'L12H24', 'L12H25', 'L12H26', 'L12H27', 'L12H28', 'L12H29', 'L12H30', 'L12H31', 'L13H0', 'L13H1', 'L13H2', 'L13H3', 'L13H4', 'L13H5', 'L13H6', 'L13H7', 'L13H8', 'L13H9', 'L13H10', 'L13H11', 'L13H12', 'L13H13', 'L13H14', 'L13H15', 'L13H16', 'L13H17', 'L13H18', 'L13H19', 'L13H20', 'L13H21', 'L13H22', 'L13H23', 'L13H24', 'L13H25', 'L13H26', 'L13H27', 'L13H28', 'L13H29', 'L13H30', 'L13H31', 'L14H0', 'L14H1', 'L14H2', 'L14H3', 'L14H4', 'L14H5', 'L14H6', 'L14H7', 'L14H8', 'L14H9', 'L14H10', 'L14H11', 'L14H12', 'L14H13', 'L14H14', 'L14H15', 'L14H16', 'L14H17', 'L14H18', 'L14H19', 'L14H20', 'L14H21', 'L14H22', 'L14H23', 'L14H24', 'L14H25', 'L14H26', 'L14H27', 'L14H28', 'L14H29', 'L14H30', 'L14H31', 'L15H0', 'L15H1', 'L15H2', 'L15H3', 'L15H4', 'L15H5', 'L15H6', 'L15H7', 'L15H8', 'L15H9', 'L15H10', 'L15H11', 'L15H12', 'L15H13', 'L15H14', 'L15H15', 'L15H16', 'L15H17', 'L15H18', 'L15H19', 'L15H20', 'L15H21', 'L15H22', 'L15H23', 'L15H24', 'L15H25', 'L15H26', 'L15H27', 'L15H28', 'L15H29', 'L15H30', 'L15H31', '0_mlp_out', '1_mlp_out', '2_mlp_out', '3_mlp_out', '4_mlp_out', '5_mlp_out', '6_mlp_out', '7_mlp_out', '8_mlp_out', '9_mlp_out', '10_mlp_out', '11_mlp_out', '12_mlp_out', '13_mlp_out', '14_mlp_out', '15_mlp_out', 'embed', 'bias']
Map:   0%|          | 0/1480 [00:00<?, ? examples/s]Map:   0%|          | 0/1480 [00:00<?, ? examples/s]
Traceback (most recent call last):
  File "/home/g/grimmj/IdiomHeads/compute_logit_attr.py", line 47, in <module>
    data.map(lambda batch: scorer.compute_logit_attr_batched(batch, grouped_file), batched=True, batch_size = batch_size)
  File "/home/g/grimmj/miniconda3/envs/idiom/lib/python3.12/site-packages/datasets/arrow_dataset.py", line 562, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g/grimmj/miniconda3/envs/idiom/lib/python3.12/site-packages/datasets/arrow_dataset.py", line 3079, in map
    for rank, done, content in Dataset._map_single(**dataset_kwargs):
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g/grimmj/miniconda3/envs/idiom/lib/python3.12/site-packages/datasets/arrow_dataset.py", line 3519, in _map_single
    for i, batch in iter_outputs(shard_iterable):
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g/grimmj/miniconda3/envs/idiom/lib/python3.12/site-packages/datasets/arrow_dataset.py", line 3469, in iter_outputs
    yield i, apply_function(example, i, offset=offset)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g/grimmj/miniconda3/envs/idiom/lib/python3.12/site-packages/datasets/arrow_dataset.py", line 3392, in apply_function
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g/grimmj/IdiomHeads/compute_logit_attr.py", line 47, in <lambda>
    data.map(lambda batch: scorer.compute_logit_attr_batched(batch, grouped_file), batched=True, batch_size = batch_size)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g/grimmj/IdiomHeads/logit_attribution.py", line 110, in compute_logit_attr_batched
    sent_score = self.compute_logit_attr(batch["sentence"][i])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g/grimmj/IdiomHeads/logit_attribution.py", line 94, in compute_logit_attr
    logit_attr = self.logit_attrs(tokens, has_batch_dim=False)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g/grimmj/IdiomHeads/logit_attribution.py", line 234, in logit_attrs
    logit_attrs = (scaled_residual_stack * logit_directions).sum(dim=-1)
                   ~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 940.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 769.88 MiB is free. Including non-PyTorch memory, this process has 6.84 GiB memory in use. Of the allocated memory 5.68 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
