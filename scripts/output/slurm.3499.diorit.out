slurmstepd-diorit: error: Unable to create TMPDIR [/tmp/user/32202]: Permission denied
slurmstepd-diorit: error: Setting TMPDIR to /tmp
2025-05-13 15:58:54.648 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.
2025-05-13 15:58:55.339 
  [33m[1mWarning:[0m to view this Streamlit app on a browser, run it with the following
  command:

    streamlit run compute_contribution.py [ARGUMENTS]
2025-05-13 15:58:55.339 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.
2025-05-13 15:58:55.339 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.
2025-05-13 15:58:55.840 Thread 'Thread-1': missing ScriptRunContext! This warning can be ignored when running in bare mode.
2025-05-13 15:58:55.840 Thread 'Thread-1': missing ScriptRunContext! This warning can be ignored when running in bare mode.
Loaded pretrained model meta-llama/Llama-3.2-1B-Instruct into HookedTransformer
2025-05-13 15:59:21.246 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.
2025-05-13 15:59:21.246 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.
Running compute_contribution on device cuda.

Processing split:  trans
Map:   0%|          | 0/2760 [00:00<?, ? examples/s]Map:   0%|          | 1/2760 [00:00<27:47,  1.65 examples/s]Map:   0%|          | 2/2760 [00:00<15:06,  3.04 examples/s]Map:   0%|          | 3/2760 [00:00<11:08,  4.12 examples/s]Map:   0%|          | 4/2760 [00:02<29:22,  1.56 examples/s]Map:   0%|          | 5/2760 [00:02<23:23,  1.96 examples/s]Map:   0%|          | 6/2760 [00:02<17:21,  2.64 examples/s]Map:   0%|          | 7/2760 [00:02<14:17,  3.21 examples/s]Map:   0%|          | 8/2760 [00:02<12:46,  3.59 examples/s]Map:   0%|          | 9/2760 [00:03<10:25,  4.40 examples/s]Map:   0%|          | 10/2760 [00:03<10:29,  4.37 examples/s]Map:   0%|          | 10/2760 [00:03<16:55,  2.71 examples/s]
Traceback (most recent call last):
  File "/home/g/grimmj/IdiomHeads/compute_contribution.py", line 49, in <module>
    data.map(lambda batch: scorer.compute_contribution_batched(batch, grouped_file), batched=True, batch_size = batch_size)
  File "/home/g/grimmj/miniconda3/envs/idiom/lib/python3.12/site-packages/datasets/arrow_dataset.py", line 562, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g/grimmj/miniconda3/envs/idiom/lib/python3.12/site-packages/datasets/arrow_dataset.py", line 3079, in map
    for rank, done, content in Dataset._map_single(**dataset_kwargs):
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g/grimmj/miniconda3/envs/idiom/lib/python3.12/site-packages/datasets/arrow_dataset.py", line 3519, in _map_single
    for i, batch in iter_outputs(shard_iterable):
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g/grimmj/miniconda3/envs/idiom/lib/python3.12/site-packages/datasets/arrow_dataset.py", line 3469, in iter_outputs
    yield i, apply_function(example, i, offset=offset)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g/grimmj/miniconda3/envs/idiom/lib/python3.12/site-packages/datasets/arrow_dataset.py", line 3392, in apply_function
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g/grimmj/IdiomHeads/compute_contribution.py", line 49, in <lambda>
    data.map(lambda batch: scorer.compute_contribution_batched(batch, grouped_file), batched=True, batch_size = batch_size)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g/grimmj/IdiomHeads/contribution.py", line 73, in compute_contribution_batched
    batch_split_contr[i] = self.compute_contribution(batch["sentence"][i], batch["idiom_pos"][i])
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g/grimmj/IdiomHeads/contribution.py", line 101, in compute_contribution
    head_contrib, _ = get_attention_contributions(resid_pre, resid_mid, decomposed_attn)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g/grimmj/miniconda3/envs/idiom/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/g/grimmj/IdiomHeads/llm-transparency-tool/llm_transparency_tool/routes/contributions.py", line 116, in get_attention_contributions
    attn_contribution, residual_contribution = get_contributions_with_one_off_part(
                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g/grimmj/miniconda3/envs/idiom/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/g/grimmj/IdiomHeads/llm-transparency-tool/llm_transparency_tool/routes/contributions.py", line 82, in get_contributions_with_one_off_part
    contributions = get_contributions(flat, whole, distance_norm)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g/grimmj/miniconda3/envs/idiom/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/g/grimmj/IdiomHeads/llm-transparency-tool/llm_transparency_tool/routes/contributions.py", line 45, in get_contributions
    distance = torch.nn.functional.pairwise_distance(parts, bc_whole, p=distance_norm)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 970.00 MiB. GPU 0 has a total capacity of 7.58 GiB of which 610.75 MiB is free. Including non-PyTorch memory, this process has 6.86 GiB memory in use. Of the allocated memory 6.06 GiB is allocated by PyTorch, and 706.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
