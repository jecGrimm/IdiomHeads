slurmstepd-andesit: error: Unable to create TMPDIR [/tmp/user/32202]: Permission denied
slurmstepd-andesit: error: Setting TMPDIR to /tmp
2025-05-13 15:29:13.222 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.
2025-05-13 15:29:13.317 
  [33m[1mWarning:[0m to view this Streamlit app on a browser, run it with the following
  command:

    streamlit run compute_contribution.py [ARGUMENTS]
2025-05-13 15:29:13.317 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.
2025-05-13 15:29:13.317 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.
2025-05-13 15:29:13.818 Thread 'Thread-1': missing ScriptRunContext! This warning can be ignored when running in bare mode.
2025-05-13 15:29:13.818 Thread 'Thread-1': missing ScriptRunContext! This warning can be ignored when running in bare mode.
Loaded pretrained model meta-llama/Llama-3.2-1B-Instruct into HookedTransformer
2025-05-13 15:29:40.199 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.
2025-05-13 15:29:40.199 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.
Running compute_contribution on device cuda.

Processing split:  formal
Map:   0%|          | 0/2760 [00:00<?, ? examples/s]Map:   0%|          | 1/2760 [00:00<28:56,  1.59 examples/s]Map:   0%|          | 2/2760 [00:00<15:20,  3.00 examples/s]Map:   0%|          | 3/2760 [00:00<10:47,  4.25 examples/s]Map:   0%|          | 4/2760 [00:01<23:56,  1.92 examples/s]Map:   0%|          | 5/2760 [00:02<19:42,  2.33 examples/s]Map:   0%|          | 6/2760 [00:02<14:43,  3.12 examples/s]Map:   0%|          | 7/2760 [00:02<12:20,  3.72 examples/s]Map:   0%|          | 8/2760 [00:02<11:10,  4.11 examples/s]Map:   0%|          | 9/2760 [00:02<09:19,  4.92 examples/s]Map:   0%|          | 10/2760 [00:02<09:23,  4.88 examples/s]Map:   0%|          | 11/2760 [00:04<35:28,  1.29 examples/s]Map:   0%|          | 12/2760 [00:05<27:59,  1.64 examples/s]Map:   0%|          | 13/2760 [00:05<21:47,  2.10 examples/s]Map:   1%|          | 14/2760 [00:05<18:14,  2.51 examples/s]Map:   1%|          | 15/2760 [00:05<15:53,  2.88 examples/s]Map:   1%|          | 16/2760 [00:06<21:29,  2.13 examples/s]Map:   1%|          | 17/2760 [00:06<17:02,  2.68 examples/s]Map:   1%|          | 18/2760 [00:08<33:44,  1.35 examples/s]Map:   1%|          | 19/2760 [00:08<25:05,  1.82 examples/s]Map:   1%|          | 20/2760 [00:08<19:42,  2.32 examples/s]Map:   1%|          | 21/2760 [00:08<15:28,  2.95 examples/s]Map:   1%|          | 22/2760 [00:08<12:27,  3.66 examples/s]Map:   1%|          | 23/2760 [00:09<14:34,  3.13 examples/s]Map:   1%|          | 24/2760 [00:09<12:09,  3.75 examples/s]Map:   1%|          | 25/2760 [00:09<10:13,  4.46 examples/s]Map:   1%|          | 26/2760 [00:09<10:48,  4.22 examples/s]Map:   1%|          | 27/2760 [00:09<10:05,  4.52 examples/s]Map:   1%|          | 28/2760 [00:10<14:27,  3.15 examples/s]Map:   1%|          | 29/2760 [00:10<15:39,  2.91 examples/s]Map:   1%|          | 30/2760 [00:11<12:54,  3.52 examples/s]Map:   1%|          | 31/2760 [00:11<10:35,  4.30 examples/s]Map:   1%|          | 32/2760 [00:11<09:36,  4.73 examples/s]Map:   1%|          | 33/2760 [00:11<08:30,  5.35 examples/s]Map:   1%|          | 34/2760 [00:11<10:19,  4.40 examples/s]Map:   1%|▏         | 35/2760 [00:12<10:28,  4.34 examples/s]Map:   1%|▏         | 36/2760 [00:12<09:09,  4.96 examples/s]Map:   1%|▏         | 38/2760 [00:12<06:51,  6.61 examples/s]Map:   1%|▏         | 39/2760 [00:12<06:51,  6.61 examples/s]Map:   1%|▏         | 40/2760 [00:13<12:50,  3.53 examples/s]Map:   1%|▏         | 41/2760 [00:13<15:08,  2.99 examples/s]Map:   2%|▏         | 42/2760 [00:13<13:07,  3.45 examples/s]Map:   2%|▏         | 43/2760 [00:14<17:24,  2.60 examples/s]Map:   2%|▏         | 44/2760 [00:14<14:22,  3.15 examples/s]Map:   2%|▏         | 45/2760 [00:14<12:06,  3.74 examples/s]Map:   2%|▏         | 47/2760 [00:14<09:41,  4.67 examples/s]Map:   2%|▏         | 48/2760 [00:15<09:04,  4.99 examples/s]Map:   2%|▏         | 50/2760 [00:15<07:18,  6.18 examples/s]Map:   2%|▏         | 51/2760 [00:16<14:45,  3.06 examples/s]Map:   2%|▏         | 52/2760 [00:16<12:21,  3.65 examples/s]Map:   2%|▏         | 53/2760 [00:16<10:19,  4.37 examples/s]Map:   2%|▏         | 54/2760 [00:16<09:00,  5.00 examples/s]Map:   2%|▏         | 55/2760 [00:16<09:18,  4.84 examples/s]Map:   2%|▏         | 56/2760 [00:17<09:44,  4.63 examples/s]Map:   2%|▏         | 57/2760 [00:17<08:36,  5.23 examples/s]Map:   2%|▏         | 58/2760 [00:17<09:16,  4.85 examples/s]Map:   2%|▏         | 59/2760 [00:17<12:46,  3.52 examples/s]Map:   2%|▏         | 60/2760 [00:18<11:05,  4.05 examples/s]Map:   2%|▏         | 61/2760 [00:18<09:28,  4.75 examples/s]Map:   2%|▏         | 62/2760 [00:18<09:40,  4.65 examples/s]Map:   2%|▏         | 63/2760 [00:18<08:12,  5.48 examples/s]Map:   2%|▏         | 64/2760 [00:18<07:47,  5.77 examples/s]Map:   2%|▏         | 65/2760 [00:18<07:35,  5.92 examples/s]Map:   2%|▏         | 66/2760 [00:19<12:10,  3.69 examples/s]Map:   2%|▏         | 67/2760 [00:19<10:05,  4.45 examples/s]Map:   2%|▏         | 68/2760 [00:19<09:26,  4.75 examples/s]Map:   2%|▎         | 69/2760 [00:19<08:10,  5.48 examples/s]Map:   3%|▎         | 70/2760 [00:21<33:34,  1.34 examples/s]Map:   3%|▎         | 71/2760 [00:21<25:26,  1.76 examples/s]Map:   3%|▎         | 73/2760 [00:22<16:16,  2.75 examples/s]Map:   3%|▎         | 74/2760 [00:22<14:13,  3.15 examples/s]Map:   3%|▎         | 75/2760 [00:22<14:51,  3.01 examples/s]Map:   3%|▎         | 76/2760 [00:23<14:40,  3.05 examples/s]Map:   3%|▎         | 77/2760 [00:23<12:12,  3.66 examples/s]Map:   3%|▎         | 78/2760 [00:23<10:03,  4.44 examples/s]Map:   3%|▎         | 79/2760 [00:23<08:31,  5.24 examples/s]Map:   3%|▎         | 80/2760 [00:23<07:27,  5.99 examples/s]Map:   3%|▎         | 81/2760 [00:23<08:08,  5.48 examples/s]Map:   3%|▎         | 82/2760 [00:23<08:47,  5.07 examples/s]Map:   3%|▎         | 83/2760 [00:24<08:30,  5.24 examples/s]Map:   3%|▎         | 84/2760 [00:24<09:43,  4.59 examples/s]Map:   3%|▎         | 85/2760 [00:24<08:48,  5.06 examples/s]Map:   3%|▎         | 86/2760 [00:24<11:22,  3.92 examples/s]Map:   3%|▎         | 87/2760 [00:25<12:34,  3.54 examples/s]Map:   3%|▎         | 88/2760 [00:25<11:05,  4.02 examples/s]Map:   3%|▎         | 89/2760 [00:25<09:39,  4.61 examples/s]Map:   3%|▎         | 90/2760 [00:25<08:37,  5.16 examples/s]Map:   3%|▎         | 91/2760 [00:26<08:58,  4.95 examples/s]Map:   3%|▎         | 92/2760 [00:26<07:51,  5.66 examples/s]Map:   3%|▎         | 93/2760 [00:26<12:14,  3.63 examples/s]Map:   3%|▎         | 94/2760 [00:27<14:10,  3.14 examples/s]Map:   3%|▎         | 95/2760 [00:27<16:04,  2.76 examples/s]Map:   3%|▎         | 96/2760 [00:27<13:03,  3.40 examples/s]Map:   3%|▎         | 96/2760 [00:28<13:00,  3.41 examples/s]
Traceback (most recent call last):
  File "/home/g/grimmj/IdiomHeads/compute_contribution.py", line 49, in <module>
    data.map(lambda batch: scorer.compute_contribution_batched(batch, grouped_file), batched=True, batch_size = batch_size)
  File "/home/g/grimmj/miniconda3/envs/idiom/lib/python3.12/site-packages/datasets/arrow_dataset.py", line 562, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g/grimmj/miniconda3/envs/idiom/lib/python3.12/site-packages/datasets/arrow_dataset.py", line 3079, in map
    for rank, done, content in Dataset._map_single(**dataset_kwargs):
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g/grimmj/miniconda3/envs/idiom/lib/python3.12/site-packages/datasets/arrow_dataset.py", line 3519, in _map_single
    for i, batch in iter_outputs(shard_iterable):
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g/grimmj/miniconda3/envs/idiom/lib/python3.12/site-packages/datasets/arrow_dataset.py", line 3469, in iter_outputs
    yield i, apply_function(example, i, offset=offset)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g/grimmj/miniconda3/envs/idiom/lib/python3.12/site-packages/datasets/arrow_dataset.py", line 3392, in apply_function
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g/grimmj/IdiomHeads/compute_contribution.py", line 49, in <lambda>
    data.map(lambda batch: scorer.compute_contribution_batched(batch, grouped_file), batched=True, batch_size = batch_size)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g/grimmj/IdiomHeads/contribution.py", line 73, in compute_contribution_batched
    batch_split_contr[i] = self.compute_contribution(batch["sentence"][i], batch["idiom_pos"][i])
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g/grimmj/IdiomHeads/contribution.py", line 101, in compute_contribution
    head_contrib, _ = get_attention_contributions(resid_pre, resid_mid, decomposed_attn)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g/grimmj/miniconda3/envs/idiom/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/g/grimmj/IdiomHeads/llm-transparency-tool/llm_transparency_tool/routes/contributions.py", line 116, in get_attention_contributions
    attn_contribution, residual_contribution = get_contributions_with_one_off_part(
                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g/grimmj/miniconda3/envs/idiom/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/g/grimmj/IdiomHeads/llm-transparency-tool/llm_transparency_tool/routes/contributions.py", line 80, in get_contributions_with_one_off_part
    flat = parts.flatten(start_dim=0, end_dim=k - 1)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.03 GiB. GPU 0 has a total capacity of 7.60 GiB of which 961.88 MiB is free. Including non-PyTorch memory, this process has 6.65 GiB memory in use. Of the allocated memory 5.45 GiB is allocated by PyTorch, and 1.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
